# LLM Configuration for Test Case Generation (Phase 6)

# LLM Provider: 'groq' or 'ollama'
LLM_PROVIDER=ollama

# Ollama Configuration (Local LLM via Docker - Recommended for local development)
# Start Ollama: bash setup_ollama.sh OR docker-compose up -d ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# Groq Configuration (Cloud API - Fast and Free tier available)
# Get your API key from: https://console.groq.com/keys
# To use Groq, set LLM_PROVIDER=groq and add your API key below
GROQ_API_KEY=
GROQ_MODEL=llama-3.3-70b-versatile
GROQ_BASE_URL=https://api.groq.com/openai/v1

# Generation Parameters
GENERATION_TEMPERATURE=0.2

# Chunking Configuration
MAX_TEXT_SIZE=5000000  # 5MB default - maximum text size to process
CHUNK_SIZE=1000        # Size of each chunk in characters
CHUNK_OVERLAP=200      # Overlap between chunks
